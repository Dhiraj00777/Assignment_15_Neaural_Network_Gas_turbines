# -*- coding: utf-8 -*-
"""Assignment_16_Neaural_Network_Gas_turbines.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W_XwstE-Q0V4w7mcNa6yflYQJh3S7dJs
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as npd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings(action='ignore')
from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split


#Load data
df = pd.read_csv('gas_turbines.csv')
df.head()

df.shape

df.columns

df.info()

df.describe().T

"""**EDA & Feature Engineering**"""

#check for misssing values
df.isna().sum()

df.isna().any()

plt.rcParams['figure.figsize']=(12,6)
sns.heatmap(df.isna(), cmap =('gist_heat'), yticklabels=False)

#check for duplicate values
df[df.duplicated()].shape

df[df.duplicated()]

df.dtypes

df.nunique()

"""Observation:
- No missing values
- No duplicate values
- All dtypes are correct.

**Data Visualisation**
"""

#Target variable
plt.title('Distplot for TEY', fontsize=17, y = 1.01)
sns.distplot(df['TEY'])

plt.title('Distplot for AT', fontsize=17, y = 1.01)
sns.distplot(df['AT'])

plt.title('Distplot for AP', fontsize=17, y = 1.01)
sns.distplot(df['AP'])

plt.title('Distplot for AH', fontsize=17, y = 1.01)
sns.distplot(df['AH'])

plt.title('Distplot for AFDP', fontsize=17, y = 1.01)
sns.distplot(df['AFDP'])

plt.title('Distplot for GTEP', fontsize=17, y = 1.01)
sns.distplot(df['GTEP'])

plt.title('Distplot for TIT', fontsize=17, y = 1.01)
sns.distplot(df['TIT'])

plt.title('Distplot for TAT', fontsize=17, y = 1.01)
sns.distplot(df['TAT'])

plt.title('Distplot for CDP', fontsize=17, y = 1.01)
sns.distplot(df['CDP'])

plt.title('Distplot for CO', fontsize=17, y = 1.01)
sns.distplot(df['CO'])

plt.title('Distplot for NOX', fontsize=17, y = 1.01)
sns.distplot(df['NOX'])

#check for outliers
fig, ax=plt.subplots(3,4, figsize=(19,6), sharex= False, sharey = False)
sns.boxplot(df.TEY, ax=ax[0,0])
sns.boxplot(df.AT, ax=ax[0,1])
sns.boxplot(df.AP, ax=ax[0,2])
sns.boxplot(df.AH, ax=ax[0,3])
sns.boxplot(df.AFDP, ax=ax[1,0])
sns.boxplot(df.GTEP, ax=ax[1,1])
sns.boxplot(df.TIT, ax=ax[1,2])
sns.boxplot(df.TAT, ax=ax[1,3])
sns.boxplot(df.CDP, ax=ax[2,0])
sns.boxplot(df.CO, ax=ax[2,1])
sns.boxplot(df.NOX, ax=ax[2,2])
plt.suptitle("Boxplot for Continuous Variables", fontsize= 17, y = 1.06)
plt.tight_layout(pad=2.0)

"""- We have a noisy data."""

import numpy as np
fig, ax=plt.subplots(4,3, figsize=(19,6), sharex= False, sharey = False)
sns.boxplot(np.log(df.TEY), ax=ax[0,0])
sns.boxplot(np.log(df.AT), ax=ax[0,1])
sns.boxplot(np.log(df.AP), ax=ax[0,2])
sns.boxplot(np.log(df.AH), ax=ax[1,0])
sns.boxplot(np.log(df.AFDP), ax=ax[1,1])
sns.boxplot(np.log(df.GTEP), ax=ax[1,2])
sns.boxplot(np.log(df.TIT), ax=ax[2,0])
sns.boxplot(np.log(df.TAT), ax=ax[2,1])
sns.boxplot(np.log(df.CDP), ax=ax[2,2])
sns.boxplot(np.log(df.CO), ax=ax[3,0])
sns.boxplot(np.log(df.NOX), ax=ax[3,1])
plt.suptitle("Log Transformation for Continuous Variables", fontsize= 17, y = 1.06)
plt.tight_layout(pad=2.0)

fig, ax=plt.subplots(3,4, figsize=(19,6), sharex= False, sharey = False)
sns.boxplot(np.sqrt(df.TEY), ax=ax[0,0])
sns.boxplot(np.sqrt(df.AT), ax=ax[0,1])
sns.boxplot(np.sqrt(df.AP), ax=ax[0,2])
sns.boxplot(np.sqrt(df.AH), ax=ax[0,3])
sns.boxplot(np.sqrt(df.AFDP), ax=ax[1,0])
sns.boxplot(np.sqrt(df.GTEP), ax=ax[1,1])
sns.boxplot(np.sqrt(df.TIT), ax=ax[1,2])
sns.boxplot(np.sqrt(df.TAT), ax=ax[1,3])
sns.boxplot(np.sqrt(df.CDP), ax=ax[2,0])
sns.boxplot(np.sqrt(df.CO), ax=ax[2,1])
sns.boxplot(np.sqrt(df.NOX), ax=ax[2,2])
plt.suptitle("SQRT Transformation for Continuous Variables", fontsize= 17, y = 1.06)
plt.tight_layout(pad=2.0)

fig, ax=plt.subplots(4,3, figsize=(19,6), sharex= False, sharey = False)
sns.boxplot(np.cbrt(df.TEY), ax=ax[0,0])
sns.boxplot(np.cbrt(df.AT), ax=ax[0,1])
sns.boxplot(np.cbrt(df.AP), ax=ax[0,2])
sns.boxplot(np.cbrt(df.AH), ax=ax[1,0])
sns.boxplot(np.cbrt(df.AFDP), ax=ax[1,1])
sns.boxplot(np.cbrt(df.GTEP), ax=ax[1,2])
sns.boxplot(np.cbrt(df.TIT), ax=ax[2,0])
sns.boxplot(np.cbrt(df.TAT), ax=ax[2,1])
sns.boxplot(np.cbrt(df.CDP), ax=ax[2,2])
sns.boxplot(np.cbrt(df.CO), ax=ax[3,0])
sns.boxplot(np.cbrt(df.NOX), ax=ax[3,1])
plt.suptitle("Cbrt Transformation for Continuous Variables", fontsize= 17, y = 1.06)
plt.tight_layout(pad=2.0)

"""- None of the transformations are helpful to treat the outliers.

**Dependency of Target variable on diff Features**
"""

sns.pairplot(df)

corr = pd.DataFrame(data = df.corr().iloc[:,7], index=df.columns)
corr = corr.sort_values(by='TEY', ascending=False)
corr

plt.title("Correlation plot between Target variables and independent variables", y=1.01, fontsize=18)
sns.barplot(x = corr.index, y = corr.TEY)

fig= plt.figure(figsize=(18, 10))
sns.heatmap(df.corr(), annot=True);
plt.xticks(rotation=45)
plt.title("Correlation Map of variables", fontsize=19)

!pip install ppscore

import ppscore as PPS
score = PPS.matrix(df)
score_s = score[score['y']=='TEY']
score_s.sort_values(by="ppscore", ascending=False)

plt.rcParams['figure.figsize']=(19,6)
sns.barplot(x='x', y='ppscore', data=score_s.sort_values(by='ppscore', ascending=False))
plt.title("PPScore of each feature with Target variable", fontsize=17, y=1.01)

"""**Observation:**
- From correlation matrix as well as ppscore we can clearly see that TEY is highly dependent on 'CDP', 'GTEP', 'TIT'.
- We can drop 'AT', 'AP', 'AH', 'NOX' as they have very less impact on dependent variables.

**Check for outliers**
"""

#check for outliers
from sklearn.ensemble import IsolationForest
data1=df.copy()

#training the model
clf = IsolationForest(random_state=10, contamination=.001)
clf.fit(data1)
data1['anamoly'] = clf.predict(data1.iloc[:,0:11])
outliers = data1[data1['anamoly']==-1]

outliers

"""- These are the outliers in our data.

**Data Preprocessing**
"""

df.shape

#drop the outliers
df = df.drop(outliers.index)
df.shape

#reset index after dropping outliers
df = df.reset_index()
df = df.drop('index', axis = 1)
df

df = df.drop(['AT', 'AP', 'AH', 'NOX'], axis=1)

df.shape

"""**Converting independent features into normalised and standardized data**"""

#Standardize & Normalize the data
norm = MinMaxScaler()
std = StandardScaler()

df_norm = pd.DataFrame(norm.fit_transform(df), columns=df.columns)            #data between -3 to +3
df_std = pd.DataFrame(std.fit_transform(df), columns=df.columns)            #data between -1 to +1

"""**Take a smaller sample to build a model**"""

#we will take a small model as this is large data and will take huge amount of time to build model
#to reandomly shuffle and select a % of data
temp = df_std.sample(frac=1)          #shuffle all the data
temp_s = df_std.sample(frac=0.1)      #shuffle and select only 10% of the data randomly to train

temp_s

"""**Splitting data into target variable and independent variables**"""

x = temp_s.drop('TEY', axis=1)
y = temp_s['TEY']
x

"""**Creating train and test data for model validation**"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)

x_train.shape, x_test.shape, y_train.shape, y_test.shape

"""**Build a Model**"""

# Importing the necessary packages
import tensorflow as tf
import keras
from sklearn.model_selection import GridSearchCV, KFold
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
from tensorflow.keras.optimizers import Adam
from keras.layers import Dropout
tf.config.experimental.list_physical_devices('GPU')               #to use GPU for faster processing of model

# create model with 2 hidden layers
def create_model_two_hidden_layers():
    model = Sequential()
    model.add(Dense(5, input_dim=6, kernel_initializer='uniform', activation='relu'))
    model.add(Dense(6, kernel_initializer='uniform', activation='relu'))
    model.add(Dense(10, kernel_initializer='uniform', activation='relu'))
    model.add(Dense(1))
    
    adam=Adam(lr=0.001)
    model.compile(loss='mse', optimizer=adam, metrics=['mse', 'mae', 'mape'])
    return model

model1 = create_model_two_hidden_layers()
print("Here is the summary of the model:")
model1.summary()

#create a model with 3 hidden layers
def create_model_three_hidden_layers():
    model = Sequential()
    model.add(Dense(32, input_dim=6, kernel_initializer='uniform', activation='relu'))
    model.add(Dense(32, kernel_initializer='uniform', activation='relu'))
    model.add(Dense(64, kernel_initializer='uniform', activation='relu'))
    model.add(Dense(128, kernel_initializer='uniform', activation='relu'))
    model.add(Dense(1))
    
    adam=Adam(lr=0.01)
    model.compile(loss='mse', optimizer=adam, metrics=['mse', 'mae', 'mape'])
    return model

model2 = create_model_three_hidden_layers()
print("Here is the summary of the model2:")
model2.summary()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# epochs=500
# batch_size=50
# 
# print("Here is the summary of this model:")
# model2.summary()
# 
# with tf.device('/GPU:0'):
#   model2.fit(x_train,y_train, verbose = 0,batch_size = batch_size,epochs = epochs, shuffle=True)

print("Predicted values:")
model2.predict(x_test[:10])

print('Actual values')
y_test[:10]

loss, mae, mse, mape = model2.evaluate(x_train, y_train)
print('\n', "Results for model 2:", '\n', "Training Loss:", loss, '\n', "Training Mean Absolute Error:" , mae, '\n', "Training Mean Squared Error:", mse)

loss, mae, mse, mape = model2.evaluate(x_test, y_test)
print('\n', "Results for model 2:", '\n', "Test Loss:", loss, '\n', "Test Mean Absolute Error:" , mae, '\n', "Test Mean Squared Error:", mse)

"""**Observations:**

- We got pretty good results for this model.
- Train and test errors are also quiet similar, which means our model is not overfitted or underfitted. 
- Still we will try to get best results by doing hyperparameter tuning.

**Hyperparameter Tuning to get best options for:** 
- batchsize
- epochs
- neurons
- learning rate
- dropout
- kernel initializer
- activation function
"""

# Create the model
#get best value for batch size and epochs by hyperparameter tuning
model = KerasRegressor(build_fn = create_model_three_hidden_layers,verbose = 0)
# Define the grid search parameters
batch_size = [30,50,70]
epochs = [300,500,800]
# Make a dictionary of the grid search parameters
param_grid = dict(batch_size = batch_size,epochs = epochs)
# Build and fit the GridSearchCV
grid = GridSearchCV(estimator = model,param_grid = param_grid,cv = KFold(),verbose = 10)
grid_result = grid.fit(x_train,y_train)

# Summarize the results
print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
  print('{},{} with: {}'.format(mean, stdev, param))

# Commented out IPython magic to ensure Python compatibility.
# #get best value for learning rate and dropuout by hyperparameter tuning
# 
# # Defining the model
# %%time
# def create_model_three_hidden_layers(learning_rate,dropout_rate):
#     model = Sequential()
#     model.add(Dense(32,input_dim = 6,kernel_initializer = 'uniform',activation = 'relu'))
#     model.add(Dropout(dropout_rate))
#     model.add(Dense(32,kernel_initializer = 'uniform',activation = 'relu'))
#     model.add(Dropout(dropout_rate))
#     model.add(Dense(64,kernel_initializer = 'uniform',activation = 'relu'))
#     model.add(Dropout(dropout_rate))
#     model.add(Dense(128,kernel_initializer = 'uniform',activation = 'relu'))
#     model.add(Dropout(dropout_rate))
#     model.add(Dense(1))
#     
#     adam = Adam(lr = learning_rate)
#     model.compile(loss = 'mse', optimizer = adam,metrics = ['mse', 'mae', 'mape'])
#     return model
# 
# # Create the model
# 
# model = KerasRegressor(build_fn = create_model_three_hidden_layers,verbose = 0,batch_size = 70,epochs = 300)
# 
# # Define the grid search parameters
# 
# learning_rate = [0.001,0.01,0.1]
# dropout_rate = [0.0,0.1,0.2]
# 
# # Make a dictionary of the grid search parameters
# 
# param_grids = dict(learning_rate = learning_rate,dropout_rate = dropout_rate)
# 
# # Build and fit the GridSearchCV
# 
# grid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 0)
# grid_result = grid.fit(x_train,y_train)

# Summarize the results
print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
  print('{},{} with: {}'.format(mean, stdev, param))

# Commented out IPython magic to ensure Python compatibility.
# # Defining the model
# #get best value for kernel initializer and activation func by hyperparameter tuning
# %%time
# def create_model_three_hidden_layers(activation_function,init):
#     model = Sequential()
#     model.add(Dense(32,input_dim = 6,kernel_initializer = init,activation = activation_function))
# 
#     model.add(Dense(32,kernel_initializer = init,activation = activation_function))
#     
#     model.add(Dense(64,kernel_initializer = init,activation = activation_function))
#     
#     model.add(Dense(128,kernel_initializer = init,activation = activation_function))
#     
#     model.add(Dense(1))
#     
#     adam = Adam(lr = 0.001)
#     model.compile(loss = 'mse',optimizer = adam,metrics = ['mse', 'mae', 'mape'])
#     return model
# 
# # Create the model
# 
# model = KerasRegressor(build_fn = create_model_three_hidden_layers,verbose = 0,batch_size = 70,epochs = 300)
# 
# # Define the grid search parameters
# activation_function = ['softmax','relu','tanh','linear']
# init = ['uniform','normal','zero']
# 
# # Make a dictionary of the grid search parameters
# param_grids = dict(activation_function = activation_function,init = init)
# 
# # Build and fit the GridSearchCV
# 
# grid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 0)
# grid_result = grid.fit(x_train,y_train)
#

# Summarize the results
print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
  print('{},{} with: {}'.format(mean, stdev, param))

# Commented out IPython magic to ensure Python compatibility.
# # Defining the model
# #get best value for neuron by hyperparameter tuning
# %%time
# def create_model_three_hidden_layers(neuron1,neuron2,neuron3,neuron4):
#     model = Sequential()
#     model.add(Dense(neuron1,input_dim = 6,kernel_initializer = 'uniform',activation = 'relu'))
#     model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = 'uniform',activation = 'relu'))
#     model.add(Dense(neuron3,input_dim = neuron2,kernel_initializer = 'uniform',activation = 'relu'))
#     model.add(Dense(neuron4,input_dim = neuron3,kernel_initializer = 'uniform',activation = 'relu'))
#     model.add(Dense(1))
#     
#     adam = Adam(lr = 0.001)
#     model.compile(loss = 'mse',optimizer = adam,metrics = ['mse', 'mae', 'mape'])
#     return model
# 
# # Create the model
# 
# model = KerasRegressor(build_fn = create_model_three_hidden_layers,verbose = 0,batch_size = 70,epochs = 300)
# 
# # Define the grid search parameters
# 
# neuron1 = [8,16,32]
# neuron2 = [32,64,128]
# neuron3 = [32,64,128]
# neuron4 = [32,64,128]
# 
# # Make a dictionary of the grid search parameters
# 
# param_grids = dict(neuron1 = neuron1,neuron2 = neuron2, neuron3 = neuron3, neuron4 = neuron4)
# 
# # Build and fit the GridSearchCV
# 
# grid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 0)
# grid_result = grid.fit(x_train,y_train)

# Summarize the results
print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
  print('{},{} with: {}'.format(mean, stdev, param))

#create a model with 3 hidden layers with best hyperparameters
def create_model_three_hidden_layers():
    model = Sequential()
    model.add(Dense(8, input_dim=6, kernel_initializer='uniform', activation='relu'))
    model.add(Dense(128, kernel_initializer='uniform', activation='relu'))
    model.add(Dense(64, kernel_initializer='uniform', activation='relu'))
    model.add(Dense(128, kernel_initializer='uniform', activation='relu'))
    model.add(Dense(1))
    
    adam=Adam(lr=0.001)
    model.compile(loss='mse', optimizer=adam, metrics=['mse', 'mae', 'mape'])
    return model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# epochs=300
# batch_size=70
# 
# final_model=create_model_three_hidden_layers()
# 
# print("Here is the summary of our final model:")
# final_model.summary()
# 
# with tf.device('/GPU:0'):
#   final_model.fit(x_train,y_train, verbose = 0,batch_size = batch_size,epochs = epochs, shuffle=True)

loss, mae, mse, mape = final_model.evaluate(x_train, y_train)
print('\n', "Results for final model :", '\n', "Training Loss:", loss, '\n', "Training Mean Absolute Error:" , mae, '\n', "Training Mean Squared Error:", mse)

loss_t, mae_t, mse_t, mape_t = final_model.evaluate(x_test, y_test)
print('\n', "Results for final model :", '\n', "Test Loss:", loss_t, '\n', "Test Mean Absolute Error:" , mae_t, '\n', "Test Mean Squared Error:", mse_t)

"""**Predicting values from Model using same dataset**"""

# generating predictions for test data
y_predict_test = model.predict(x_test) 

# creating table with test price & predicted price for test
predictions_df = pd.DataFrame(x_test)
predictions_df['Actual'] = y_test
predictions_df['Predicted'] = y_predict_test
print(predictions_df.shape)
predictions_df.head(10)

"""**Visualizing the Relationship between the Actual and Predicted Values Model Validation**"""

plt.figure(figsize=(12,8))
plt.xlabel("Actual Values")
plt.ylabel("Predicted values")
plt.title("The Scatterplot of Relationship between Actual Values and Predictions")
plt.scatter(predictions_df['Actual'], predictions_df['Predicted'])